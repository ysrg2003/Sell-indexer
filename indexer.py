# FILE: indexer.py
# ROLE: Commercial version with Daily Limit enforcement for Google Indexing API.

import os
import json
import requests
from oauth2client.service_account import ServiceAccountCredentials

def log(message):
    print(f"[SYSTEM LOG] {message}")

SCOPES = ["https://www.googleapis.com/auth/indexing"]
ENDPOINT = "https://indexing.googleapis.com/v3/urlNotifications:publish"

def get_credentials():
    json_creds = os.getenv('GOOGLE_INDEXING_JSON')
    if not json_creds:
        log("âš ï¸ CRITICAL: GOOGLE_INDEXING_JSON is missing in Secrets.")
        return None
    
    try:
        info = json.loads(json_creds)
        return ServiceAccountCredentials.from_json_keyfile_dict(info, SCOPES)
    except Exception as e:
        log(f"âŒ AUTH ERROR: {e}")
        return None

def submit_url(url, access_token):
    try:
        headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
        content = {"url": url, "type": "URL_UPDATED"}
        
        r = requests.post(ENDPOINT, data=json.dumps(content), headers=headers)
        
        if r.status_code == 200:
            log(f"âœ… SUCCESS: {url}")
            return True
        else:
            log(f"âš ï¸ API REFUSED {url}: {r.text}")
            return False
            
    except Exception as e:
        log(f"âŒ ERROR processing {url}: {e}")
        return False

def main():
    # 1. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ Ø§Ù„ÙŠÙˆÙ…ÙŠ Ù„Ù„Ø¨Ø§Ù‚Ø© (ÙŠÙØ¬Ù„Ø¨ Ù…Ù† Secrets Ø£Ùˆ ÙŠÙƒÙˆÙ† 20 Ø§ÙØªØ±Ø§Ø¶ÙŠØ§Ù‹)
    DAILY_LIMIT = int(os.getenv('DAILY_LIMIT', 20)) 
    urls_file = "urls.txt"
    
    if not os.path.exists(urls_file):
        log("âŒ Error: urls.txt not found.")
        return

    # 2. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ù…Ù„Ù
    with open(urls_file, "r") as f:
        all_urls = [line.strip() for line in f if line.strip().startswith("http")]

    if not all_urls:
        log("âš ï¸ No URLs to process.")
        return

    # 3. ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø§Ù… Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø¨Ø§Ù‚Ø©
    total_found = len(all_urls)
    urls_to_process = all_urls[:DAILY_LIMIT] # Ø£Ø®Ø° Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø¶Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ø³Ù…ÙˆØ­ ÙÙ‚Ø·
    skipped_count = total_found - len(urls_to_process)

    log(f"ğŸ“Š Package Limit: {DAILY_LIMIT} URLs per run.")
    log(f"ğŸ“‚ Found in file: {total_found} URLs.")
    
    if skipped_count > 0:
        log(f"ğŸš« Plan Restriction: {skipped_count} URLs were skipped. Please upgrade your plan.")

    # 4. Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØµÙ„Ø§Ø­ÙŠØ§Øª Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©
    creds = get_credentials()
    if not creds: return
    
    try:
        access_token = creds.get_access_token().access_token
    except Exception as e:
        log(f"âŒ Failed to get access token: {e}")
        return

    # 5. Ø§Ù„Ø¨Ø¯Ø¡ ÙÙŠ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„
    success_count = 0
    for url in urls_to_process:
        if submit_url(url, access_token):
            success_count += 1
        
    log("-" * 30)
    log(f"ğŸ Final Report: {success_count} Success | {len(urls_to_process) - success_count} Failed")
    log(f"ğŸš€ Powered by LatestAI Indexer Pro")

if __name__ == "__main__":
    main()
